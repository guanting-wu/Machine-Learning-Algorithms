{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder = ['train/ham', 'train/spam', 'test/ham', 'test/spam']\n",
    "for path in path_folder:\n",
    "    all_file_list=os.listdir(path)\n",
    "    if(path == 'train/ham'):\n",
    "        train_ham = pd.DataFrame({'Label':['ham'] * len(all_file_list), 'Content':[0] * len(all_file_list)})\n",
    "        for i in range(len(all_file_list)):\n",
    "            try:\n",
    "                with open(path + '/' + all_file_list[i],'r', encoding = 'utf-8') as f:\n",
    "                    content = f.read()\n",
    "            except UnicodeDecodeError:\n",
    "                pass;\n",
    "            train_ham.loc[i, 'Content'] = content\n",
    "    elif(path == 'train/spam'):\n",
    "        train_spam = pd.DataFrame({'Label':['spam'] * len(all_file_list), 'Content':[0] * len(all_file_list)})\n",
    "        for i in range(len(all_file_list)):\n",
    "            try:\n",
    "                with open(path + '/' + all_file_list[i],'r', encoding = 'utf-8') as f:\n",
    "                    content = f.read()\n",
    "            except UnicodeDecodeError:\n",
    "                pass;\n",
    "            train_spam.loc[i, 'Content'] = content\n",
    "    elif(path == 'test/ham'):\n",
    "        test_ham = pd.DataFrame({'Label':['ham'] * len(all_file_list), 'Content':[0] * len(all_file_list)})\n",
    "        for i in range(len(all_file_list)):\n",
    "            try:\n",
    "                with open(path + '/' + all_file_list[i],'r', encoding = 'utf-8') as f:\n",
    "                    content = f.read()\n",
    "            except UnicodeDecodeError:\n",
    "                pass;\n",
    "            test_ham.loc[i, 'Content'] = content\n",
    "    else:\n",
    "        test_spam = pd.DataFrame({'Label':['spam'] * len(all_file_list), 'Content':[0] * len(all_file_list)})\n",
    "        for i in range(len(all_file_list)):\n",
    "            try:\n",
    "                with open(path + '/' + all_file_list[i],'r', encoding = 'utf-8') as f:\n",
    "                    content = f.read()\n",
    "            except UnicodeDecodeError:\n",
    "                pass;\n",
    "            test_spam.loc[i, 'Content'] = content    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_message(message):\n",
    "    message = message.lower()\n",
    "    message = message.replace('subject','',1) #remove the first \"subject\"\n",
    "    message = re.sub(r'\\d+', '', message) #remove numbers\n",
    "    words = word_tokenize(message)\n",
    "    words = [w for w in words if len(w) > 2]\n",
    "    #stop word\n",
    "    sw = stopwords.words('english')\n",
    "    words = [word for word in words if word not in sw]\n",
    "    #stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]   \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.concat([train_ham, train_spam], axis = 0).reset_index(drop = True)\n",
    "train_set = shuffle(train_set)\n",
    "train_set = train_set.iloc[:10000].reset_index(drop = True)\n",
    "test_set = pd.concat([test_ham, test_spam], axis = 0).reset_index(drop = True)\n",
    "test_set = shuffle(test_set)\n",
    "test_set = test_set.iloc[:2000].reset_index(drop = True)\n",
    "train_set['Content'] = train_set['Content'].apply(process_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Content</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: i can see your on dialup\\nhow are you...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: message subject\\n- - - - 158806477142...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: i want to mentor you\\nthis week i sho...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: scotty\\nkelly ,\\ngovenment don ' t wa...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: re , your pharmacy o , rder # 845235\\...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: cheap oem soft shipping worldwide\\nwh...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: success in dating - written by women ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: [ none ]\\ncraving for a luxury wwatch...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: from hajia mariam abacha and the chil...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: marketing your business via the inter...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1016 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                            Content predicted\n",
       "0     spam  Subject: i can see your on dialup\\nhow are you...      spam\n",
       "1     spam  Subject: message subject\\n- - - - 158806477142...      spam\n",
       "4     spam  Subject: i want to mentor you\\nthis week i sho...      spam\n",
       "9     spam  Subject: scotty\\nkelly ,\\ngovenment don ' t wa...      spam\n",
       "10    spam  Subject: re , your pharmacy o , rder # 845235\\...      spam\n",
       "...    ...                                                ...       ...\n",
       "1980  spam  Subject: cheap oem soft shipping worldwide\\nwh...      spam\n",
       "1986  spam  Subject: success in dating - written by women ...      spam\n",
       "1987  spam  Subject: [ none ]\\ncraving for a luxury wwatch...      spam\n",
       "1991  spam  Subject: from hajia mariam abacha and the chil...      spam\n",
       "1995  spam  Subject: marketing your business via the inter...      spam\n",
       "\n",
       "[1016 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[test_set.Label == 'spam']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "for content in train_set['Content']:\n",
    "    for word in content:\n",
    "        vocabulary.append(word)\n",
    "\n",
    "vocabulary = list(set(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60298"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_per_content = {unique_word: [0] * len(train_set['Content']) for unique_word in vocabulary}\n",
    "\n",
    "for index, content in enumerate(train_set['Content']):\n",
    "    for word in content:\n",
    "        word_counts_per_content[word][index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brean</th>\n",
       "      <th>kio</th>\n",
       "      <th>bettor</th>\n",
       "      <th>criterionalp</th>\n",
       "      <th>cuidad</th>\n",
       "      <th>rip</th>\n",
       "      <th>product</th>\n",
       "      <th>niamh</th>\n",
       "      <th>waken</th>\n",
       "      <th>arteriol</th>\n",
       "      <th>...</th>\n",
       "      <th>woud</th>\n",
       "      <th>wyrazici</th>\n",
       "      <th>blacklin</th>\n",
       "      <th>mtg</th>\n",
       "      <th>ujnti</th>\n",
       "      <th>riskbenc</th>\n",
       "      <th>stripteas</th>\n",
       "      <th>ormoan</th>\n",
       "      <th>melendez</th>\n",
       "      <th>leopard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60298 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   brean  kio  bettor  criterionalp  cuidad  rip  product  niamh  waken  \\\n",
       "0      0    0       0             0       0    0        0      0      0   \n",
       "1      0    0       0             0       0    0        0      0      0   \n",
       "2      0    0       0             0       0    0        0      0      0   \n",
       "3      0    0       0             0       0    0        0      0      0   \n",
       "4      0    0       0             0       0    0        0      0      0   \n",
       "\n",
       "   arteriol  ...  woud  wyrazici  blacklin  mtg  ujnti  riskbenc  stripteas  \\\n",
       "0         0  ...     0         0         0    0      0         0          0   \n",
       "1         0  ...     0         0         0    0      0         0          0   \n",
       "2         0  ...     0         0         0    0      0         0          0   \n",
       "3         0  ...     0         0         0    0      0         0          0   \n",
       "4         0  ...     0         0         0    0      0         0          0   \n",
       "\n",
       "   ormoan  melendez  leopard  \n",
       "0       0         0        0  \n",
       "1       0         0        0  \n",
       "2       0         0        0  \n",
       "3       0         0        0  \n",
       "4       0         0        0  \n",
       "\n",
       "[5 rows x 60298 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = pd.DataFrame(word_counts_per_content)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Content</th>\n",
       "      <th>brean</th>\n",
       "      <th>kio</th>\n",
       "      <th>bettor</th>\n",
       "      <th>criterionalp</th>\n",
       "      <th>cuidad</th>\n",
       "      <th>rip</th>\n",
       "      <th>product</th>\n",
       "      <th>niamh</th>\n",
       "      <th>...</th>\n",
       "      <th>woud</th>\n",
       "      <th>wyrazici</th>\n",
       "      <th>blacklin</th>\n",
       "      <th>mtg</th>\n",
       "      <th>ujnti</th>\n",
       "      <th>riskbenc</th>\n",
       "      <th>stripteas</th>\n",
       "      <th>ormoan</th>\n",
       "      <th>melendez</th>\n",
       "      <th>leopard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[california, capac, report, week, transwestern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[remind, real, time, interview, today, jake, a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>[aussi, save, medlcatlon, pharma, rectangular,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>[minut, hard, rock, ever, stuck, outrag, presc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>[ambllen, alprazzolam, aluum, llgra, caall, le...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                            Content  brean  kio  \\\n",
       "0   ham  [california, capac, report, week, transwestern...      0    0   \n",
       "1   ham  [remind, real, time, interview, today, jake, a...      0    0   \n",
       "2  spam  [aussi, save, medlcatlon, pharma, rectangular,...      0    0   \n",
       "3  spam  [minut, hard, rock, ever, stuck, outrag, presc...      0    0   \n",
       "4  spam  [ambllen, alprazzolam, aluum, llgra, caall, le...      0    0   \n",
       "\n",
       "   bettor  criterionalp  cuidad  rip  product  niamh  ...  woud  wyrazici  \\\n",
       "0       0             0       0    0        0      0  ...     0         0   \n",
       "1       0             0       0    0        0      0  ...     0         0   \n",
       "2       0             0       0    0        0      0  ...     0         0   \n",
       "3       0             0       0    0        0      0  ...     0         0   \n",
       "4       0             0       0    0        0      0  ...     0         0   \n",
       "\n",
       "   blacklin  mtg  ujnti  riskbenc  stripteas  ormoan  melendez  leopard  \n",
       "0         0    0      0         0          0       0         0        0  \n",
       "1         0    0      0         0          0       0         0        0  \n",
       "2         0    0      0         0          0       0         0        0  \n",
       "3         0    0      0         0          0       0         0        0  \n",
       "4         0    0      0         0          0       0         0        0  \n",
       "\n",
       "[5 rows x 60300 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_clean = pd.concat([train_set, word_counts], axis=1)\n",
    "train_set_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolating spam and ham messages first\n",
    "spam_messages = train_set_clean[train_set_clean['Label'] == 'spam']\n",
    "ham_messages = train_set_clean[train_set_clean['Label'] == 'ham']\n",
    "\n",
    "# P(Spam) and P(Ham)\n",
    "p_spam = len(spam_messages) / len(train_set_clean)\n",
    "p_ham = len(ham_messages) / len(train_set_clean)\n",
    "\n",
    "# N_Spam\n",
    "n_words_per_spam_message = spam_messages['Content'].apply(len)\n",
    "n_spam = n_words_per_spam_message.sum()\n",
    "\n",
    "# N_Ham\n",
    "n_words_per_ham_message = ham_messages['Content'].apply(len)\n",
    "n_ham = n_words_per_ham_message.sum()\n",
    "\n",
    "# N_Vocabulary\n",
    "n_vocabulary = len(vocabulary)\n",
    "\n",
    "# Laplace smoothing\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate parameters\n",
    "parameters_spam = {unique_word:0 for unique_word in vocabulary}\n",
    "parameters_ham = {unique_word:0 for unique_word in vocabulary}\n",
    "\n",
    "# Calculate parameters\n",
    "for word in vocabulary:\n",
    "    n_word_given_spam = spam_messages[word].sum() # spam_messages already defined\n",
    "    p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha*n_vocabulary)\n",
    "    parameters_spam[word] = p_word_given_spam\n",
    "\n",
    "    n_word_given_ham = ham_messages[word].sum() # ham_messages already defined\n",
    "    p_word_given_ham = (n_word_given_ham + alpha) / (n_ham + alpha*n_vocabulary)\n",
    "    parameters_ham[word] = p_word_given_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(message):\n",
    "    '''\n",
    "    message: a string\n",
    "    '''\n",
    "\n",
    "    message = process_message(message)\n",
    "    \n",
    "    h_nb_spam = np.log(p_spam)\n",
    "    h_nb_ham = np.long(p_ham)\n",
    "\n",
    "    for word in message:\n",
    "        if word in parameters_spam:\n",
    "            h_nb_spam += np.log(parameters_spam[word])\n",
    "            \n",
    "        if word in parameters_ham: \n",
    "            h_nb_ham += np.log(parameters_ham[word])\n",
    "\n",
    "    if (h_nb_ham > h_nb_spam):\n",
    "        return 'ham'\n",
    "    elif (h_nb_ham < h_nb_spam):\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'needs human classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Content</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: i can see your on dialup\\nhow are you...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: message subject\\n- - - - 158806477142...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : credit . com cv ' s\\nshirley ,\\n...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: enron mentions\\nenron taps $ 3 billio...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: i want to mentor you\\nthis week i sho...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                            Content predicted\n",
       "0  spam  Subject: i can see your on dialup\\nhow are you...      spam\n",
       "1  spam  Subject: message subject\\n- - - - 158806477142...      spam\n",
       "2   ham  Subject: re : credit . com cv ' s\\nshirley ,\\n...       ham\n",
       "3   ham  Subject: enron mentions\\nenron taps $ 3 billio...       ham\n",
       "4  spam  Subject: i want to mentor you\\nthis week i sho...      spam"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set['predicted'] = test_set['Content'].apply(classify)\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 1947\n",
      "Incorrect: 53\n",
      "Accuracy: 0.9735\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = test_set.shape[0]\n",
    "\n",
    "for row in test_set.iterrows():\n",
    "    row = row[1]\n",
    "    if row['Label'] == row['predicted']:\n",
    "        correct += 1\n",
    "\n",
    "print('Correct:', correct)\n",
    "print('Incorrect:', total - correct)\n",
    "print('Accuracy:', correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
